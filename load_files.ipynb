{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import dgl\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBL_dir = r'OBL/HQ_DIR/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A:\\Anaconda3\\envs\\ML37\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "def namestr(obj, namespace):\n",
    "    return [name for name in namespace if namespace[name] is obj]\n",
    "\n",
    "def add_df_info_to_dict(df_info, df):\n",
    "    df_info[namestr(df, globals())[0]] = {'shape': df.shape, 'columns': list(df.columns)}\n",
    "\n",
    "def add_df_info_to_dict(df_info, df):\n",
    "    df_info[namestr(df, globals())[0]] = {'shape': df.shape, 'columns': df.columns}\n",
    "    \n",
    "df_info = dict()\n",
    "\n",
    "#EDGES\n",
    "edges = pd.read_csv(os.path.join(OBL_dir, 'graph_files/', 'edges.csv'),\n",
    "                    header=None,\n",
    "                    sep='\\t',\n",
    "                    names = ['node_id_1', 'edge_type', 'node_id_2', 'quality_score', 'source'])\n",
    "add_df_info_to_dict(df_info, edges)   \n",
    "\n",
    "edges_list = pd.read_csv(os.path.join(OBL_dir, 'graph_files/', 'edges_list.csv'),\n",
    "                    header=None,\n",
    "                    sep='\\t',\n",
    "                    names = ['edge_type'])\n",
    "add_df_info_to_dict(df_info, edges_list) \n",
    "\n",
    "#TN EDGES\n",
    "TN_edges = pd.read_csv(os.path.join(OBL_dir, 'graph_files/', 'TN_edges.csv'),\n",
    "                    header=None,\n",
    "                    sep='\\t',\n",
    "                    names = ['node_id_1', 'edge_type', 'node_id_2', 'quality_score', 'source'])\n",
    "add_df_info_to_dict(df_info, TN_edges) \n",
    "\n",
    "TN_edges_list = pd.read_csv(os.path.join(OBL_dir, 'graph_files/', 'TN_edges_list.csv'),\n",
    "                    header=None,\n",
    "                    sep='\\t',\n",
    "                    names = ['edge_type'])\n",
    "add_df_info_to_dict(df_info, TN_edges_list) \n",
    "\n",
    "#NODES\n",
    "all_nodes = pd.read_csv(os.path.join(OBL_dir, 'graph_files/', 'ALL_nodes.csv'),\n",
    "                    header=None,\n",
    "                    sep='\\t',\n",
    "                    names = ['node_id', 'node_type']) \n",
    "add_df_info_to_dict(df_info, all_nodes) \n",
    "\n",
    "nodes = pd.read_csv(os.path.join(OBL_dir, 'graph_files/', 'nodes.csv'),\n",
    "                    header=None,\n",
    "                    sep='\\t',\n",
    "                    names = ['node_id', 'node_type'])\n",
    "add_df_info_to_dict(df_info, nodes)\n",
    "\n",
    "nodes_list = pd.read_csv(os.path.join(OBL_dir, 'graph_files/', 'nodes_list.csv'),\n",
    "                    header=None,\n",
    "                    sep='\\t',\n",
    "                    names = ['nodes_list'])\n",
    "add_df_info_to_dict(df_info, nodes_list) \n",
    "\n",
    "#TN NODES\n",
    "TN_nodes = pd.read_csv(os.path.join(OBL_dir, 'graph_files/', 'TN_nodes.csv'),\n",
    "                    header=None,\n",
    "                    sep='\\t',\n",
    "                    names = ['node_id', 'node_type'])\n",
    "add_df_info_to_dict(df_info, TN_nodes) \n",
    "\n",
    "TN_nodes_list = pd.read_csv(os.path.join(OBL_dir, 'graph_files/', 'TN_nodes_list.csv'),\n",
    "                    header=None,\n",
    "                    sep='\\t',\n",
    "                    names = ['nodes_list'])\n",
    "add_df_info_to_dict(df_info, TN_nodes_list) \n",
    "\n",
    "#NO MAPPING\n",
    "ids_no_mapping = pd.read_csv(os.path.join(OBL_dir, 'graph_files/', 'ids_no_mapping.tsv'),\n",
    "                    header=None,\n",
    "                    sep='\\t',\n",
    "                    names = ['node_id', 'node_type'])\n",
    "add_df_info_to_dict(df_info, ids_no_mapping) \n",
    "\n",
    "TN_ids_no_mapping = pd.read_csv(os.path.join(OBL_dir, 'graph_files/', 'tn_ids_no_mapping.tsv'),\n",
    "                    header=None,\n",
    "                    sep='\\t',\n",
    "                    names = ['node_id', 'node_type'])\n",
    "add_df_info_to_dict(df_info, TN_ids_no_mapping) \n",
    "\n",
    "#STATS\n",
    "stats = pd.read_csv(os.path.join(OBL_dir, 'graph_files/', 'stats.txt'),\n",
    "                    sep = '\\t',\n",
    "                    header=[0])\n",
    "add_df_info_to_dict(df_info, stats) \n",
    "\n",
    "tn_stats = pd.read_csv(os.path.join(OBL_dir, 'graph_files/', 'tn_stats.txt'),\n",
    "                    sep = '\\t',\n",
    "                    header=[0])\n",
    "add_df_info_to_dict(df_info, tn_stats) \n",
    "\n",
    "#TRAIN/VAL/TEST/SPLIT\n",
    "#POSITIVE EDGES\n",
    "train_sample = pd.read_csv(os.path.join(OBL_dir, 'train_test_data/', 'train_sample.csv'),\n",
    "                    sep = '\\t',\n",
    "                    header=None,\n",
    "                    names = ['node_id_1', 'edge_type', 'node_id_2', 'quality_score', 'label', 'source'])\n",
    "add_df_info_to_dict(df_info, train_sample) \n",
    "\n",
    "val_sample = pd.read_csv(os.path.join(OBL_dir, 'train_test_data/', 'val_sample.csv'),\n",
    "                    sep = '\\t',\n",
    "                    header=None,\n",
    "                    names = ['node_id_1', 'edge_type', 'node_id_2', 'quality_score', 'label', 'source'])\n",
    "add_df_info_to_dict(df_info, val_sample) \n",
    "\n",
    "test_sample = pd.read_csv(os.path.join(OBL_dir, 'train_test_data/', 'test_sample.csv'),\n",
    "                    sep = '\\t',\n",
    "                    header=None,\n",
    "                    names = ['node_id_1', 'edge_type', 'node_id_2', 'quality_score', 'label', 'source'])\n",
    "add_df_info_to_dict(df_info, test_sample) \n",
    "\n",
    "#NEGATIVE EDGES\n",
    "negative_test_sample = pd.read_csv(os.path.join(OBL_dir, 'train_test_data/', 'negative_test_sample.csv'),\n",
    "                    sep = '\\t',\n",
    "                    header=None,\n",
    "                    names = ['node_id_1', 'edge_type', 'node_id_2', 'quality_score', 'label', 'source'])\n",
    "add_df_info_to_dict(df_info, negative_test_sample) \n",
    "\n",
    "negative_train_sample = pd.read_csv(os.path.join(OBL_dir, 'train_test_data/', 'negative_train_sample.csv'),\n",
    "                    sep = '\\t',\n",
    "                    header=None,\n",
    "                    names = ['node_id_1', 'edge_type', 'node_id_2', 'quality_score', 'label', 'source'])\n",
    "add_df_info_to_dict(df_info, negative_train_sample) \n",
    "\n",
    "negative_val_sample = pd.read_csv(os.path.join(OBL_dir, 'train_test_data/', 'negative_val_sample.csv'),\n",
    "                    sep = '\\t',\n",
    "                    header=None,\n",
    "                    names = ['node_id_1', 'edge_type', 'node_id_2', 'quality_score', 'label', 'source'])\n",
    "add_df_info_to_dict(df_info, negative_val_sample) \n",
    "\n",
    "#NODES\n",
    "train_val_nodes = pd.read_csv(os.path.join(OBL_dir, 'train_test_data/', 'train_val_nodes.csv'),\n",
    "                    sep = '\\t',\n",
    "                    header=None,\n",
    "                    names = ['node_id', 'node_type'])\n",
    "add_df_info_to_dict(df_info, train_val_nodes) \n",
    "\n",
    "removed_val_nodes = pd.read_csv(os.path.join(OBL_dir, 'train_test_data/', 'removed_val_nodes.csv'),\n",
    "                    sep = '\\t',\n",
    "                    header=None,\n",
    "                    names = ['node_id'])\n",
    "add_df_info_to_dict(df_info, removed_val_nodes) \n",
    "\n",
    "removed_test_nodes = pd.read_csv(os.path.join(OBL_dir, 'train_test_data/', 'removed_test_nodes.csv'),\n",
    "                    sep = '\\t',\n",
    "                    header=None,\n",
    "                    names = ['node_id'])\n",
    "add_df_info_to_dict(df_info, removed_test_nodes) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, info in df_info.items():\n",
    "    print('---')\n",
    "    print(df_name)\n",
    "    for key, data in info.items():\n",
    "        print(key, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unique_node_id(all_nodes):\n",
    "      \n",
    "    #creates unique_id\n",
    "    all_nodes['unique_id'] = list(range(len(all_nodes)))\n",
    "    #creates unique_id based on node type\n",
    "    all_nodes['type_id'] = all_nodes.groupby(['node_type'])['node_id'].transform(lambda x: list(range(len(x))))\n",
    "\n",
    "    initial_length = len(all_nodes)\n",
    "    final_length = len(all_nodes)\n",
    "    assert initial_length == final_length, 'Final lenght should be the same as initial length'\n",
    "    \n",
    "    print(f\"Total number of unique IDs is {all_nodes['node_id'].unique().shape[0]} \")\n",
    "\n",
    "    return all_nodes\n",
    "\n",
    "def merge_node_ids_and_edges(edges,\n",
    "                             merge_on_col,\n",
    "                             cols_to_merge,\n",
    "                             node_position):\n",
    "\n",
    "    nodes_df = all_nodes.copy()\n",
    "    nodes_df = nodes_df.rename(columns={col:col + f'_{node_position}' for col in merge_on_col + cols_to_merge})\n",
    "\n",
    "    edges = pd.merge(edges,\n",
    "                     nodes_df,\n",
    "                     on=[col + f'_{node_position}' for col in merge_on_col],\n",
    "                     how='left')\n",
    "            \n",
    "    return edges\n",
    "        \n",
    "edges = pd.read_csv(os.path.join(OBL_dir, 'graph_files/', 'edges.csv'),\n",
    "                    header=None,\n",
    "                    sep='\\t',\n",
    "                    names = ['node_id_1', 'edge_type', 'node_id_2', 'quality_score', 'source'])\n",
    "\n",
    "all_nodes = create_unique_node_id(all_nodes)\n",
    "\n",
    "merge_on_col = ['node_id']\n",
    "cols_to_merge = [col for col in all_nodes.columns if col != merge_on_col]\n",
    "\n",
    "edges = merge_node_ids_and_edges(edges,\n",
    "                                 merge_on_col,\n",
    "                                 cols_to_merge,\n",
    "                                 node_position = '1')\n",
    "\n",
    "edges = merge_node_ids_and_edges(edges,\n",
    "                                 merge_on_col,\n",
    "                                 cols_to_merge,\n",
    "                                 node_position = '2')\n",
    "\n",
    "edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
